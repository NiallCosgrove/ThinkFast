# ThinkFast

## Fine-Tuning Qwen2.5 for Efficient, Compressed Reasoning

### ğŸ“Œ Overview

ThinkFast is an experiment in fine-tuning Qwen2.5 to develop structured, compressed reasoning. Instead of enforcing verbose step-by-step explanations, this project explores whether LLMs can reason efficientlyâ€”producing the correct answer while minimizing token usage.

### ğŸ›  Features

- âœ… RLHF Training with GRPO â€“ Reinforcing structured reasoning with reward shaping.

- âœ… Reasoning Compression â€“ not yet fully implimented - optimizing thought processes.

- âœ… Unconstrained Inference Compute â€“ Testing whether extra inference cycles improve reasoning.

- âœ… Multi-GPU Capable â€“ Fully compatible with multi-GPU fine-tuning.

- âœ… Efficient GGUF Export â€“ Convert fine-tuned models for llama.cpp inference.


***This is a work in progress***


### ğŸ“Œ Quick Start

- 1. Set up env
'''
conda create -n thinkfast python=3.11 -y
conda activate thinkfast
pip install torch transformers huggingface_hub unsloth trl accelerate vllm
pip install -r requirements.txt
'''

There is a setup.sh that might work for you if you dont have conda - not fully tested  - ymmv

- 2. Run the GRPO fune tuner
'''
python grpo.py --model <hf-model-name>  --train_steps 1000
'''
- 3. gguf export is in grpo.py  - change False to True on the lines with your preferred export formats
you will need llama.cpp in your directory - todo arg parse this






## ğŸ† Attribution
This project uses portions of code from [Unsloth](https://github.com/unslothai) under the Apache 2.0 license.
Some training loop components and techniques were adapted from Unslothâ€™s blog posts and codebase.
See the [`LICENSE`](LICENSE) file for details.

Apache2.0 Licence
